\begin{markdown}

## Programming model ##



CUDA and OpenCL offer a SIMT model for exploiting _flat_ data
parallelism. The programmer is required to write kernels that can
execute as indepentend theads. Through the runtime system and the use
of indexing the model ensures that each execution of the kernel works
on different parts of the input data structure.

### Data Parallel Haskell ###

In \cite{DPH} DPH, Data Parallel Haskell, is presented. DPH is an
extension to Haskell for declarative parallel programming. DPH adds
only three concepts to the Haskell to provide a model for parallel
programming. These are examplfied in Table \ref{tab:dph}

\end{markdown}
\end{multicols}
\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    Category&Concept&Example\\
    \hline \hline
    Datatype & Parallel arrays & $[:e:]$, where $e$ is a type \\
    \hline
    Operations & Parallel operations & $mapP$, $filterP$, $zipP$, ... \\
    \hline
    Syntactic Sugar & Parallel Array Comprehensions & $[: i | i <- [1,2,3,4] :]$ \\
    \hline
  \end{tabular}
  \caption{DPH Programming Model}
  \label{tab:dph}
\end{table}


\begin{multicols}{2}
\begin{markdown}

Using the features in Table \ref{tab:dph} the programmer can define
parallel computations on nested datastructures and the compiler will
take the appropiate measure to distribute the computations on the
underlying hardware.

### Producing efficient code ###

In order to produce efficient code given a description in DPH the
compiler goes through four steps outlined in Figure \ref{tab:dph-steps}.

\begin{figure}[H]
  \begin{enumerate}
    \item Desugaring
    \item Vectorisation
    \item Fusion
    \item Gang Parallelism
  \end{enumerate}
  \caption{DPH Compiler steps}
  \label{tab:dph-steps}
\end{figure}

#### Desugaring ####

In the desugaring phase the syntax presented in Table \ref{tab:dph} is
lowered into GHC Core language. This mainly consists of tranforming
array comprehensions into high-order functions like $mapP$ and $zipP$.

#### Vectorisation ####

The vectorisation step consists of creating vector versions of the
scalar functions used. The lifted version is generated by vectorising
the implementations of the function. The terminology in the paper is
to lift the function. Denoted by $f_L$ for the lifted version of
$f$. _The key in the method_, to flatten nested data structures, is
handled in this step. Nested computations is usualy handled by
recursive functions. Naivly this would lead to the need to lift the
lifted version of a function and so on. The method handles this with
the insight from Blellock \cite{nesl} given in Figure \ref{fig:lift2}.

\end{markdown}
\begin{figure}[H]
  $$ f_{LL}(xss) = unconcat(xss,\ f_L(concat,\ xss)) $$
  \caption{Definition of $f_{LL}$}
  \label{fig:lift2}
\end{figure}

\begin{markdown}
#### Fusion ####

The vectorisation approach described in the paper produces a lot of
overhead due to intermediate arrays and redundant
synchronistaions. This overhead is eliminated in the fusion step where
compiler optimizations consisting of rewrite rules and inlineing.

#### Gang Parallelism ####

The scheduling of the code is done in the SPMD model. Here the work
load is divided up in threads, one for each PE, Processing Element, of
the system. A few explicit primitives for distrubution is introduced
in Table \ref{tab:dph-dist}.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    $splitD$ & Distributes array into one chunck per thread \\
    \hline
    $mapD$ & Executes a function on a distributed array \\
    \hline
    $joinD$ & Join a distributed array into a global array \\
    \hline
  \end{tabular}
  \caption{DPH Distibution primitives}
  \label{tab:dph-dist}
\end{table}

\end{markdown}
